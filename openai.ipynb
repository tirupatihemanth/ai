{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPENAI API\n",
    "- API Documentation at https://platform.openai.com/docs/api-reference\n",
    "- python openai lib documentation: https://platform.openai.com/docs/guides/gpt\n",
    "- https://colab.research.google.com to access GPUs & also write ipynb code.\n",
    "- Developing AI applications on unstructured data is where LLMs excel. For structured data like spreadsheets/tables this doesn't work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "# openai.Model.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0):\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\":\"user\", \"content\":prompt}\n",
    "        ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature = temperature, # Degree of randomness of model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def get_completion_from_messages(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To guide a model towards the desired output and reduce irrelevant or incorrect responses, it is important to provide clear and specific instructions, which may require longer prompts for more clarity and context.\n"
     ]
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "You should express what you want a model to do by \\\n",
    "providing instructions that are as clear and \\\n",
    "specific as you can possibly make them. \\\n",
    "This will guide the model towards the desired output, \\\n",
    "and reduce the chances of receiving irrelevant \\\n",
    "or incorrect responses. Don't confuse writing a \\\n",
    "clear prompt with writing a short prompt. \\\n",
    "In many cases, longer prompts provide more clarity \\\n",
    "and context for the model, which can lead to \\\n",
    "more detailed and relevant outputs.\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Summarize the text delimited by triple backticks \\\n",
    "into a single sentence. \\\n",
    "```{text}``` \\\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
